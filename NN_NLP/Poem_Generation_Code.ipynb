{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb96e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bfc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43a5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a65d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bff693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"G:\\ML_FCC\\LLM from Scratch\\input_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbdb487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20657 poems in the dataset\n"
     ]
    }
   ],
   "source": [
    "poems = get_text_files(path, folders = ['forms','topics'])\n",
    "print(\"There are\",len(poems),\"poems in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a016e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 ballads in the dataset\n"
     ]
    }
   ],
   "source": [
    "ballads = get_text_files(path+'/forms', folders = ['ballad'])\n",
    "print(\"There are\",len(ballads),\"ballads in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14df2893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.â€™\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "source": [
    "txt = poems[0].open().read(); \n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b832f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballad_texts = []\n",
    "\n",
    "# Iterate over each file and read its content\n",
    "for file_path in ballads:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        try:\n",
    "            content = file.read().decode('utf-8')  # Use the appropriate encoding\n",
    "            ballad_texts.append(content)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Error decoding file: {file_path}\")\n",
    "\n",
    "# Convert the list of ballad texts into a numpy array\n",
    "ballads_array = np.array(ballad_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa40f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72b5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ballads = flatten(ballads_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b5f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [range_of(70), range(100)]\n",
    "tls = TfmdLists(all_ballads, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9787a92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The burden of hard hitting. Slug away\r\n",
      "Like Honus Wagner or like Tyrus Cobb.\r\n",
      "Else fandom shouteth: \"Who said you could play?\r\n",
      "Back to the jasper league, you minor slob!\"\r\n",
      "Swat, hit, connect, line out, goet on the job.\r\n",
      "Else you shall feel the brunt of fandom's ire\r\n",
      "Biff, bang it, clout it, hit it on the knob -\r\n",
      "This is the end of every fan's desire.\r\n",
      "The burden of good pitching. Curved or straight.\r\n",
      "Or in or out, or haply up or down,\r\n",
      "To puzzle him that standeth by the plate,\r\n",
      "To lessen, so to speak, his bat-renown:\r\n",
      "Like Christy Mathewson or Miner Brown,\r\n",
      "So pitch that every man can but admire\r\n",
      "And offer you the freedom of the town -\r\n",
      "This is the end of every fan's desire.\r\n",
      "The burden of loud cheering. O the sounds!\r\n",
      "The tumult and the shouting from the throats\r\n",
      "Of forty thousand at the Polo Grounds\r\n",
      "Sitting, ay, standing sans their hats and coats.\r\n",
      "A mighty cheer that possibly denotes\r\n",
      "That Cub or Pirate fat is in the fire;\r\n",
      "Or, as H. James would say, We've got their goats -\r\n",
      "This is the end of every fan's desire.\r\n",
      "The burden of a pennant. O the hope,\r\n",
      "The tenuous hope, the hope that's half a fear,\r\n",
      "The lengthy season and the boundless dope,\r\n",
      "And the bromidic, \"Wait until next year.\"\r\n",
      "O dread disgrace of trailing in the rear,\r\n",
      "O Piece of Bunting, flying high and higher\r\n",
      "That next October it shall flutter here:\r\n",
      "This is the end of every fan's desire.\r\n",
      "ENVOY\r\n",
      "Ah, Fans, let not the Quarry but the Chase\r\n",
      "Be that to which most fondly we aspire!\r\n",
      "For us not Stake, but Game; not Goal, but Race -\r\n",
      "THIS is the end of every fan's desire.\n"
     ]
    }
   ],
   "source": [
    "show_at(tls.train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fad94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "bs,sl = 4,256\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6110dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have walked through tougher Harlem where few strangers dare to go\\r\\nAnd I've been in London City in the rain and in the snow\\r\\nAnd I've worked in inner Melbourne in the searing summer heat\\r\\nAnd believe me if I tell you I have earned the bread I eat.\\r\\nI have laboured in deep trenches with my life I've took a dare\\r\\nAnd I've worked in cherry pickers ninety foot up in the air\\r\\nAnd the hands of time keep turning and the years go quickly by\\r\\nAnd the man who lives on welfare is still better off than I.\\r\\nAnd who needs the tag of good worker it's no big deal anyway\\r\\nHe's a wiser and better off man who sits at home all day\\r\\nAnd his conscience doesn't prick him isn't he the lucky one\\r\\nAnd must I be one great idiot to go labouring in the sun.\\r\\nI was low in social ladder and I still am way</td>\n",
       "      <td>have walked through tougher Harlem where few strangers dare to go\\r\\nAnd I've been in London City in the rain and in the snow\\r\\nAnd I've worked in inner Melbourne in the searing summer heat\\r\\nAnd believe me if I tell you I have earned the bread I eat.\\r\\nI have laboured in deep trenches with my life I've took a dare\\r\\nAnd I've worked in cherry pickers ninety foot up in the air\\r\\nAnd the hands of time keep turning and the years go quickly by\\r\\nAnd the man who lives on welfare is still better off than I.\\r\\nAnd who needs the tag of good worker it's no big deal anyway\\r\\nHe's a wiser and better off man who sits at home all day\\r\\nAnd his conscience doesn't prick him isn't he the lucky one\\r\\nAnd must I be one great idiot to go labouring in the sun.\\r\\nI was low in social ladder and I still am way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>body\\r\\nYou should not do this thing.\\r\\n\"Take off, take off, those shoes of pride,\\r\\nCarry them whence they came;\\r\\nYour Captains saw your insolence,\\r\\nAnd they shall see your shame.\"\\r\\nWhen Mehtab Singh came to the door\\r\\nHis shoes they burned his hand,\\r\\nFor there in long and silent lines\\r\\nHe saw the Captains stand.\\r\\nWhen Mehtab Singh rode from the gate\\r\\nHis chin was on his breast:\\r\\nThe captains said, \"When the strong command\\r\\nObedience is best.\"'T was Fultah Fisher's boarding-house,\\r\\nWhere sailor-men reside,\\r\\nAnd there were men of all the ports\\r\\nFrom Mississip to Clyde,\\r\\nAnd regally they spat and smoked,\\r\\nAnd fearsomely they lied.\\r\\nThey lied about the purple Sea\\r\\nThat gave them scanty bread,\\r\\nThey lied about the Earth beneath,\\r\\nThe Heavens overhead,\\r\\nFor they had looked too often on\\r\\nBlack rum when that was red.\\r\\nThey told their tales of wreck and wrong,\\r\\nOf shame and lust and fraud,</td>\n",
       "      <td>\\r\\nYou should not do this thing.\\r\\n\"Take off, take off, those shoes of pride,\\r\\nCarry them whence they came;\\r\\nYour Captains saw your insolence,\\r\\nAnd they shall see your shame.\"\\r\\nWhen Mehtab Singh came to the door\\r\\nHis shoes they burned his hand,\\r\\nFor there in long and silent lines\\r\\nHe saw the Captains stand.\\r\\nWhen Mehtab Singh rode from the gate\\r\\nHis chin was on his breast:\\r\\nThe captains said, \"When the strong command\\r\\nObedience is best.\"'T was Fultah Fisher's boarding-house,\\r\\nWhere sailor-men reside,\\r\\nAnd there were men of all the ports\\r\\nFrom Mississip to Clyde,\\r\\nAnd regally they spat and smoked,\\r\\nAnd fearsomely they lied.\\r\\nThey lied about the purple Sea\\r\\nThat gave them scanty bread,\\r\\nThey lied about the Earth beneath,\\r\\nThe Heavens overhead,\\r\\nFor they had looked too often on\\r\\nBlack rum when that was red.\\r\\nThey told their tales of wreck and wrong,\\r\\nOf shame and lust and fraud,\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02805198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fb2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6adee6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Python\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "F:\\Python\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [5.849992752075195,347.23187255859375]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac1763d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241m.\u001b[39mlr_find()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'love is ridiculous'\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = torch.tensor(prompt_ids)[None]  # No need for .cuda() here\n",
    "\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.model.generate(inp, max_length=60, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(preds[0].cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ef724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "def generate_text(prompt):\n",
    "    prompt_ids = tokenizer.encode(prompt)\n",
    "    inp = torch.tensor(prompt_ids)[None]\n",
    "\n",
    "    preds = model.generate(\n",
    "        inp,\n",
    "        max_length=100,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(preds[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=gr.inputs.Textbox(prompt=\"Enter a prompt\"),\n",
    "    outputs=\"text\",\n",
    "    live=True,\n",
    "    title=\"GPT-2 Text Generation\",\n",
    "    description=\"Generate text using GPT-2 model\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8173d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(Callback):\n",
    "    def after_epoch(self):\n",
    "        print(f\"Epoch {self.epoch}: Train Loss - {self.learn.recorder.losses[-1]:.4f}, Perplexity - {self.learn.recorder.metrics[-1]:.2f}\")\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='valid_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
    "learn = language_model_learner(\n",
    "    dls, \n",
    "    AWD_LSTM, \n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    cbs=[DropOutput, CustomCallback],\n",
    "    metrics=Perplexity(),\n",
    "    opt_func=AdamW,\n",
    "    lr=1e-3\n",
    ").to_fp16()\n",
    "learn.fit_one_cycle(5, lr_max=slice(1e-3))\n",
    "learn.lr_find()\n",
    "learn.fit_one_cycle(5, lr_max=slice(1e-5, 1e-4), cbs=lr_scheduler)\n",
    "learn.save(\"fine_tuned_model\")\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = learn.predict(prompt, n_words=50, temperature=0.8)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a245171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
