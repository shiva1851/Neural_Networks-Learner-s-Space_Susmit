{"cells":[{"cell_type":"markdown","metadata":{"id":"pbwA7e12AVLe"},"source":["# **Sentiment Analysis using LSTM**\n","In this week's graded assignment, you will implement a neural network that will perform senyiment analysis on movie reviews and classify them as positive or negative.\n","\n","You are to only write/modify the code in between consecutive `# <START>` and `# <END>` comments. DO NOT modify other parts of the notebook, your assignments will not be graded otherwise.\n","\n","```python\n","\"Don't modify any code here\"\n","\n","# <START>\n","\"YOUR CODE GOES HERE!\"\n","# <END>\n","\n","\"Don't modify any code here\"\n","```\n","## **Before you begin**\n","Before you start with the assignment, you will have to upload the CSV file containing the movie reviews you will classify to your Google Drive. Here's a link to the dataset: [Dataset Link](https://drive.google.com/file/d/1nqmfdx7dj5qgynVwzD1CMjFFZoBeKrmD/view?usp=sharing)  \n","Download **reviews.csv** and upload it to your Drive.\n","\n","### **Make sure to upload to the same account that you are using on Colab, otherwise you won't be able to access the files**\n"]},{"cell_type":"markdown","metadata":{"id":"D0lIe1uahq_L"},"source":["## **Mounting Google Drive**\n","First, we need to import the dataset from your Google Drive. To do so, **run the below cell**. This will mount your Drive to the running Colab instance. Then, you will be able to access all your Google Drive data in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObnNbRJPQrYK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690559282110,"user_tz":-330,"elapsed":28335,"user":{"displayName":"Susmit Neogi","userId":"13611694697015027714"}},"outputId":"12756a5f-b0fb-4efa-ecb8-9d4bcef0d156"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ElYLMOGFBq1r"},"source":["## **Import the Libraries**\n","\n","Run the cell below to import all the necessary libraries for building the sentiment analysis model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNUw4hS8BrbK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690559303294,"user_tz":-330,"elapsed":14171,"user":{"displayName":"Susmit Neogi","userId":"13611694697015027714"}},"outputId":"0a7dc2f7-deb1-46f3-d0cb-7281a8ba8575"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","\n","!pip install tqdm\n","from tqdm import tqdm\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["## **Loading the Dataset**\n","\n","The below cell contains a function `getdata`, which takes as argument the path to `reviews.csv` in your Google Drive, loads the CSV file into a Pandas DataFrame and finally returns the DataFrame.\n","\n","Complete the function and then add the path to `IMDB Dataset.csv` in the space indicated. Run the cell to load the dataset."],"metadata":{"id":"-q-caOpJ_qhU"}},{"cell_type":"code","source":["def getdata(path):\n","\n","  # <START>\n","  df = pd.read_csv(path)\n","  return df\n","  # <END>\n","\n","# Insert the path to the file in the space below\n","# <START>\n","path = '/content/drive/MyDrive/reviews.csv'\n","# <END>\n","\n","df = getdata(path)\n","df.head(5)"],"metadata":{"id":"lpaOs3EkAgJC","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1690562818559,"user_tz":-330,"elapsed":1258,"user":{"displayName":"Susmit Neogi","userId":"13611694697015027714"}},"outputId":"842658f2-4812-4046-8abc-cbcc4f699e10"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"],"text/html":["\n","\n","  <div id=\"df-fd75f92d-5a4e-4270-b237-b134fdeb5ad1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd75f92d-5a4e-4270-b237-b134fdeb5ad1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-f1572192-7f1c-4a27-9e38-426b4ce38eb4\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1572192-7f1c-4a27-9e38-426b4ce38eb4')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-f1572192-7f1c-4a27-9e38-426b4ce38eb4 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fd75f92d-5a4e-4270-b237-b134fdeb5ad1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fd75f92d-5a4e-4270-b237-b134fdeb5ad1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"GbRrtTIYBrbM"},"source":["## **Splitting the Dataset into Training and Testing Data**\n","Complete the below cell that splits the loaded data into training and testing sets. The recommended train:test ratio is 80:20, but feel free to change this later and see how it affects the accuracy of the model. Note that the cell should store the training and testing reviews in `x_train` and `x_test` respectively, and their corresponding sentiments in `y_train` and `y_test` respectively.\n","\n","<details>\n","  <summary>Hint</summary>\n","  Check out the train_test_split function from sklearn.\n","  Do not forget to shuffle the dataset!\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VAB-v2HBrbM"},"outputs":[],"source":["train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n","\n","# <START>\n","x_train =\n","x_test =\n","y_train =\n","y_test =\n","# <END>\n","\n","print(x_train.shape)\n","print(x_test.shape)"]},{"cell_type":"markdown","source":["Before we proceed, let's make sure we have a well distributed training dataset by checking the number of positive and negative reviews it contains.\n","\n","If your values deviate too much from the optimal 50-50 ratio, it is recommended that you modify the above cell to shuffle the dataset differently."],"metadata":{"id":"IOJcQL3EGVOX"}},{"cell_type":"code","source":["count_positive = 0\n","count_negative = 0\n","\n","for i in y_train:\n","  if i == \"positive\":\n","    count_positive += 1\n","  elif i ==\"negative\":\n","    count_negative += 1\n","\n","print(\"Positive reviews:\", count_positive/(count_positive + count_negative)*100, \"%\")\n","print(\"Negative reviews:\", count_negative/(count_positive + count_negative)*100, \"%\")"],"metadata":{"id":"UBm9-OJ5GiiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbF4nskOBrbN"},"source":["## **Preprocessing**\n","As you saw in last week's assignment, preprocessing a very important step in NLP. Complete the below function to perform the following tasks. The function should take as input a set of reviews `x_train` or `x_test`, and return a list of preprocessed reviews.\n","\n","\n","1.   Case the corpus to lower case\n","2.   Remove punctuation\n","3.   Lemmatize\n","4.   Remove stop words\n","\n","Feel free to refer to last week's assignment for guidance.\n","\n","*Note that as the dataset is much larger this time, the cell may take a while to run.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJn1Uqc4BrbO"},"outputs":[],"source":["#The list of stopwords has been coverted into a set, making it faster to check if a given word is in it\n","stop_words = set(stopwords.words('english'))\n","\n","# Using the below tokenizer helps us get rid of punctuation,\n","tokenizer = RegexpTokenizer(r'\\w+')\n","# And the lemmatizer to convert words to simpler forms\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess(x_set):\n","    result = []\n","    for i in x_set:\n","      # <START>\n","      out =\n","      # <END>\n","      result.append(out)\n","    return result\n","\n","#Testing the function\n","test_preprocess = preprocess([\"This is an assignment.\",'The notebook runs on the cloud.','The cloud, however, is very much on the ground,'])\n","print(test_preprocess)\n","assert test_preprocess == [['assignment'], ['notebook', 'run', 'cloud'], ['cloud', 'however', 'much', 'ground']]\n","\n","x_train_preprocessed = preprocess(x_train)\n","x_test_preprocessed = preprocess(x_test)"]},{"cell_type":"markdown","source":["## **Building the Vocabulary**\n","Complete the below function which takes as argument the preprocessed **training set** and returns a dictionary mapping each unique word to a unique ID. However this time, we shall begin with giving the first word an ID of `1`. The reason for this will become clear soon.\n","\n","Again, feel free to refer to last week's assignment."],"metadata":{"id":"SNLwSHF4Lcqu"}},{"cell_type":"code","source":["def create_vocabulary(x_train_preprocessed):\n","  '''Creates a dictionary with all unique words in corpus with id'''\n","  vocabulary = {}\n","  id = 1\n","  # <START>\n","  for s in x_train_preprocessed:\n","  # <END>\n","  return vocabulary\n","\n","vocabulary = create_vocabulary(x_train_preprocessed)\n","print(len(vocabulary))"],"metadata":{"id":"FKRU-3xDOLpx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Converting to Numerical Form**\n","Since we need to convert our words into numerical input for the neural network to be able to process it, let's replace the words in our reviews with their corresponding IDs from the vocabulary (not their one-hot embeddings, just their IDs).\n","\n","Complete the below function that takes as input the set of reviews (`x_train_preprocessed` and `x_test_preprocessed`) and converts them to their numerical form.\n"],"metadata":{"id":"oFsfs_POeMkk"}},{"cell_type":"code","source":["def convert_to_numerical(x_set):\n","  converted = []\n","    # <START>\n","  for x in x_set:\n","    # <END>\n","  return converted\n","\n","#Testing your function\n","vocab_list = list(vocabulary.keys())\n","print(convert_to_numerical([[vocab_list[47],vocab_list[186],vocab_list[4005]]]))\n","assert convert_to_numerical([[vocab_list[47],vocab_list[186],vocab_list[4005]]]) == [[48, 187, 4006]]"],"metadata":{"id":"zhjvZjcMhRHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train_numerical = convert_to_numerical(x_train_preprocessed)\n","x_test_numerical = convert_to_numerical(x_test_preprocessed)\n","\n","print(len(x_train_numerical))"],"metadata":{"id":"hkXMWld3ehG9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that our sentiment labels (`y_train` and `y_test`) currently contain the words `positive` and `negative`. Let's convert them to `1` and `0` respectively."],"metadata":{"id":"bKjdEgl3ikYE"}},{"cell_type":"code","source":["def convert_labels(y_set):\n","  converted=[]\n","    # <START>\n","  for i in y_set:\n","    # <END>\n","  return converted\n","\n","assert convert_labels(['positive','negative','positive','positive','negative']) == [1,0,1,1,0]\n","\n","y_train_numerical = convert_labels(y_train)\n","y_test_numerical = convert_labels(y_test)\n","\n","print(len(y_train_numerical))"],"metadata":{"id":"bkDKnbYHi4tm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Normalizing the Reviews' lengths**\n","Since our reviews have variable lengths, we will employ a technique called **padding**.to normalize them.\n","\n","Run the below cell to analyze the present lengths of our reviews."],"metadata":{"id":"k97nNtmeTXKn"}},{"cell_type":"code","source":["lengths = [len(i) for i in x_train_preprocessed]\n","lengths_sum = sum(lengths)\n","num_lengths = len(lengths)\n","avg_length = lengths_sum/num_lengths\n","max_length = max(lengths)\n","\n","print(\"Average length of reviews: \", avg_length)\n","print(\"Length of the longest review: \", max_length)"],"metadata":{"id":"lBpGv4w9T4AR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Padding all the reviews to the maximum length would preserve all our data, but at the same time it would be highly inefficient. Instead, let's settle on an intermediate value of 150 and pad all shorter reviews to 150. As for the longer ones, our model can probably perform sentiment analysis without the *entire* review, so we'll truncate them to 150 words.\n","\n","For this purpose, we shall use a ghost ID of `0` to represent the padding token. This is why we did not give this ID to the first word.\n","\n","Run the below cell to normalize the lengths as mentioned."],"metadata":{"id":"bveQkxZOWnS8"}},{"cell_type":"code","source":["def normalize(x_set, max_length):\n","    temp = np.zeros((len(x_set), max_length),dtype=int)\n","    for i, sentence in enumerate(x_set):\n","        if len(sentence) != 0:\n","            temp[i, -len(sentence):] = np.array(sentence)[:max_length]\n","    return temp\n","\n","max_length = 150\n","\n","x_train_normalized = normalize(x_train_numerical, max_length)\n","x_test_normalized = normalize(x_test_numerical, max_length)"],"metadata":{"id":"ouVh5UrnZKDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CEWbRjXBrbP"},"source":["## **Converting the Data to Tensors**\n","Now that we are done processing all the reviews, it's time to start building our sentiment analysis model. But first, we'll need to convert our training and testing data into a suitable form.\n","\n","This can be done using the `TensorDataset` class of PyTorch, which creates a dataset containing tensors for input features and labels.\n","\n","Since this class takes only NumPy ndarrays as parameters, we'll need to convert `y_train_numerical` and `y_test_numerical` into ndarrays."]},{"cell_type":"code","source":["final_train_data = TensorDataset(torch.from_numpy(x_train_normalized), torch.tensor(y_train_numerical))\n","final_test_data = TensorDataset(torch.from_numpy(x_test_normalized), torch.tensor(y_test_numerical))"],"metadata":{"id":"qLVPF86FnW6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Loading the Data in Batches**\n","Since our dataset is quite large, loading it all at once won't be feasible. A much more efficient way of loading the dataset is to do it in **batches**. This can be done using PyTorch's DataLoader constructor.\n","\n","The below cell creates `train_batch_loader` and `test_batch_loader`, using the variable `batch_size`.\n","\n","<details>\n","  <summary>Note</summary>\n","The shuffle = True parameter is important, omitting it would defeat the purpose of batching!</details>"],"metadata":{"id":"pjG46lSurNmF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITFt2pVBBrbP"},"outputs":[],"source":["batch_size = 64\n","\n","train_batch_loader = DataLoader(final_train_data, shuffle=True, batch_size=batch_size)\n","test_batch_loader = DataLoader(final_test_data, shuffle=True, batch_size=batch_size)"]},{"cell_type":"markdown","source":["Run the following cell to visualize an input-labels pair from a batch of the training data. Observe that the size of any set of input features is $[\\textrm{number of batches}, \\textrm{maximum (normalized) length of review}]$."],"metadata":{"id":"jdaNXixrs_ov"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sGdzClJBrbP"},"outputs":[],"source":["sample_batch = train_batch_loader\n","for x, y in sample_batch:\n","  print(\"Input size:\", x.size())\n","  print(\"Input feature: \\n\", x)\n","  print(\"Label: \\n\", y)\n","  break\n"]},{"cell_type":"markdown","metadata":{"id":"YlOP0HiBBrbP"},"source":["## **Building the Model**\n","Complete the below cell to build the RNN model. This time, we will be using **Python classes** to define the model. The class will inherit from `torch.nn.Module`, which is PyTorch's base class for all neural network modules.\n","\n","This time, the model will have 5 parts:\n","- An **Embedding** layer that converts the IDs of all the words in the vocabulary, and converts them to embedding vectors of a defined size  \n","Note that this is basically doing the job of the emdedding model from last time\n","- The **LSTM layer(s)** that will sequentially run through the data.  \n","PyTorch allows you to define multiple continuous LSTM layers with a simpler syntax than having to individually define each one\n","- The **Dropout** layer. This makes sure our model doesn't overfit the training data. With a predefined probability, it excludes certain nodes from the architecture in each training run.\n","- The **Linear** layer. This takes the predicted tokens and converts them into a single prediction on whether or not the sentiment is positive or negative.\n","- Of course, the part of making the predicition a probability is done by the **Sigmoid** layer\n","\n","The forward function has already been written for you. You are to now define these layers in the RNN object.\n","\n","For the LSTM layer, note that the current assignment implements a *batch first* approach."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xa8yqaeDBrbP"},"outputs":[],"source":["class RNN_SA(nn.Module):\n","\n","    # CLASS CONSTRUCTOR\n","    def __init__(self, num_layers, hidden_layer_size, embedding_layer_size, output_layer_size, vocabulary_size, dropout_probability=0.5):\n","        super(RNN_SA,self).__init__()\n","\n","        # Initialize the class variables with the values provided as arguments\n","        self.output_layer_size = output_layer_size\n","        self.hidden_dim = hidden_layer_size\n","        self.num_layers = num_layers\n","        self.vocabulary_size = vocabulary_size\n","\n","        # <START>\n","        # Define the embedding layer\n","        self.embedding =\n","\n","        # Define the LSTM unit\n","        self.lstm =\n","\n","        # Define the Dropout layer\n","        self.dropout_layer =\n","\n","        # Define the linear hidden layer and output sigmoid layer\n","        self.linear_layer =\n","        self.sigmoid_layer =\n","        # <END>\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        embeds = self.embedding(x)\n","        lstm_out,_ = self.lstm(embeds)\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        out = self.dropout_layer(lstm_out)\n","        out = self.linear_layer(out)\n","        sig_out = self.sigmoid_layer(out)\n","        sig_out = sig_out.view(batch_size, -1)\n","        sig_out = sig_out[:, -1]\n","        return sig_out"]},{"cell_type":"markdown","source":["Run the below cell to create a model using the class you just defined.\n","\n","<details>\n","  <summary>Why is vocabulary_size = len(vocabulary) + 1?</summary>\n","  The extra 1 is because of the ghost ID 0 we added for padding\n","</details>"],"metadata":{"id":"b6Evd1FaS857"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R_KCEx7BrbQ"},"outputs":[],"source":["num_layers = 2\n","vocabulary_size = len(vocabulary) + 1\n","embedding_layer_size = 64\n","output_layer_size = 1\n","hidden_layer_size = 256\n","\n","\n","model = RNN_SA(num_layers, hidden_layer_size, embedding_layer_size, output_layer_size, vocabulary_size, dropout_probability=0.3)\n","\n","print(model)"]},{"cell_type":"markdown","source":["Expected:\n","```console\n","RNN_SA(\n","  (embedding): Embedding(VOCAB_SIZE, 64)\n","  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n","  (dropout_layer): Dropout(p=0.3, inplace=False)\n","  (linear_layer): Linear(in_features=256, out_features=1, bias=True)\n","  (sigmoid_layer): Sigmoid()\n",")\n","```"],"metadata":{"id":"D2RgepLVT1mg"}},{"cell_type":"markdown","source":["On the off-chance that your notebook ever died mid-training, in the training section we've implemented a saving mechanism that saves your model to your Drive if it improves from the previous epoch.\n","\n","Just set the aptly names variable to `True` and run this cell. Remember to set it back to `False` afterwards."],"metadata":{"id":"-MA_HOXOYCHy"}},{"cell_type":"code","source":["MY_NOTEBOOK_DIED_MID_TRAINING = False\n","\n","if MY_NOTEBOOK_DIED_MID_TRAINING:\n","  model = torch.load('/content/drive/MyDrive/state_dict.pt')"],"metadata":{"id":"6k0rFhpcS7JD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aVdOn9zDBrbQ"},"source":["## **Training the Model**\n","Now that we have our model, let's begin training it on our training data. The hyperparameters have been defined for you. You might notice that the number of epochs is quite low; this is because each epoch takes a pretty long time to run and a model trained with even 5 epochs would take over an hour to finish training.\n","\n","Since our model is a binary classifier trained to predict 0 and 1, we can use Binary Cross Entropy loss to measure the loss.\n","\n","Unlike in the first week's assignment, you will not need to explicitly update the parameters after each run of gradient descent, we have imported an optimizer that takes care of this.\n","\n","We have also given you a function `accuracy`, that takes as arguments the predictions generated by the model and the corresponding true values, and returns the accuracy.\n"]},{"cell_type":"markdown","source":["\n","Your task is to complete the rest of the cell and train the model.\n","\n","**Again, each epoch could take upto 15 minutes to run, so please be patient!**\n","\n","If your notebook ever dies mid-training, go to the cell below this one.\n","<details>\n","  <summary>What do model.train() and model.epoch do?</summary>\n","  Checkout https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n","</details>"],"metadata":{"id":"waP6OQiNZYNt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z49izPtgBrbQ"},"outputs":[],"source":["learning_rate = 1e-3\n","num_epochs = 3\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","\n","def accuracy(prediction, label):\n","    prediction = torch.round(prediction.squeeze())\n","    return torch.sum(prediction == label.squeeze()).item()\n","\n","min_loss_attained = np.Inf    # this variable will contain the minimum value of the loss function attained in any of the epochs.\n","                              #It is used to discard the results of further epochs if their losses are more than min_loss_attained.\n","\n","epoch_train_loss,epoch_test_loss = [],[]\n","epoch_train_acc,epoch_test_acc = [],[]    # These lists will hold the average loss and accuracy values of each epoch\n","\n","\n","\n","for epoch in range(num_epochs):\n","  train_losses = 0.0\n","  train_accuracy = 0.0\n","\n","  progress_bar = tqdm(total=len(train_batch_loader), desc='Training', leave = True)  # You don't need to worry about this line,\n","                                                                                          # it's just to display a progress bar that shows the percentage trained\n","\n","  model.train()\n","\n","  for inputs, labels in train_batch_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    # <START>\n","    # Don't forget to reset the gradients before beginning!\n","    output =\n","\n","    # compute the loss and implement the back propagation step\n","    loss =\n","\n","    # <END>\n","\n","    # Calculating loss and accuracy; we will store the cumulative values and finally use them to compute the average loss and accuracy of the epoch\n","    train_losses += loss.item()\n","    acc = accuracy(output,labels)\n","    train_accuracy += acc\n","\n","    nn.utils.clip_grad_norm_(model.parameters(), 5) # This line helps avoid the exploding gradient problem\n","\n","    optimizer.step()\n","\n","    progress_bar.update(1)\n","\n","  progress_bar.close()\n","\n","  # After the training run of each epoch, we will evaluate the performance of our model\n","\n","  test_losses = 0.0  # Again, this will store cumulative loss\n","  test_accuracy = 0.0   # and this will store cumulative test accuracy\n","\n","  model.eval()\n","\n","  for inputs, labels in test_batch_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    # <START>   # Compute the loss\n","    output =\n","    test_loss =\n","    # <END>\n","\n","    test_losses += test_loss.item()\n","\n","    acc = accuracy(output,labels)\n","    test_accuracy += acc\n","\n","\n","  epoch_train_accuracy = train_accuracy/len(train_batch_loader.dataset)\n","  epoch_test_accuracy = test_accuracy/len(test_batch_loader.dataset)\n","\n","\n","  avg_train_loss = np.mean(train_losses)\n","  avg_test_loss = np.mean(test_losses)\n","\n","  epoch_train_loss.append(avg_train_loss)\n","  epoch_test_loss.append(avg_test_loss)\n","  epoch_test_acc.append(epoch_test_accuracy)\n","  epoch_train_acc.append(epoch_train_accuracy)\n","\n","  print(f'Epoch {epoch+1}')\n","  print(f'train_loss : {avg_train_loss} test_loss : {avg_test_loss}')\n","  print(f'train_accuracy : {epoch_train_accuracy*100} test_accuracy : {epoch_test_accuracy*100}')\n","  if avg_test_loss < min_loss_attained:\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/state_dict.pt')\n","    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_loss_attained, avg_test_loss))\n","    min_loss_attained = avg_test_loss\n","  print(70*'=')\n"]},{"cell_type":"markdown","source":["Let's visualize the accuracy and loss of our model graphically. Run the below cell to generate plots for the same."],"metadata":{"id":"X_UPDcpRk8HI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtl0ZSZFBrbQ"},"outputs":[],"source":["fig = plt.figure(figsize = (20, 6))\n","plt.subplot(1, 2, 1)\n","plt.plot(epoch_train_acc, label='Training Accuracy')\n","plt.plot(epoch_test_acc, label='Testing Accuracy')\n","plt.title(\"Accuracy\")\n","plt.legend()\n","plt.grid()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epoch_train_loss, label='Training loss')\n","plt.plot(epoch_test_loss, label='Testing loss')\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.grid()\n","\n","plt.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1l1dxNj0NfBgdoSiIE-f8AB_syvqYa7sK","timestamp":1690558364043},{"file_id":"1vTQ5E5IAznpP1DwSkmeqhqe7pp6GqE7U","timestamp":1690464712191},{"file_id":"1zhjsHO6ABBkz04Uo0M6cHN6dPzLn7UdU","timestamp":1690384320896}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":0}